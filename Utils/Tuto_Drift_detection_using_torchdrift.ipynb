{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [**Drift Detection Using TorchDrift for Tabular and Time-series Data**](https://pub.towardsai.net/drift-detection-using-torchdrift-c6d348ac7329)  \n",
    "### **Learn to monitor your model in production**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Introduction**  \n",
    "Machine learning models are designed to make predictions based on data. However, the data in the real world is constantly changing, and this can affect the accuracy of the model. This is known as data drift, and it can lead to incorrect predictions and poor performance. In this blog post, we will discuss how to detect data drift using the Python library TorchDrift."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **What is Data Drift?**  \n",
    "Data drift occurs when the statistical properties of the data used to train a machine-learning model change over time. This can be due to changes in the input data, changes in the distribution of the input data, or changes in the relationship between the input and output variables. Data drift can cause the model to become less accurate over time, and it is important to detect and correct it to maintain the performance of the model.\n",
    "\n",
    "To illustrate the concept, two distributions are different from each other used in a hypothetical machine learning model, which can result in poor performanace issues."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Types of drifts**  \n",
    "There are different types of drifts that can occur after the deployment of a machine-learning model. The two prominent ones are:  \n",
    "1. **Feature drift**: The distribution of one or more input variables changes over time. $P(X)$ changes but $P(y|X)$ remains the same.  \n",
    "$$\n",
    "   P(X_{train}) \\not = P(X_{test}) \\\\\n",
    "   P(y_{train}|X_{train}) = P(y_{test}|X_{test})\n",
    "$$\n",
    "2. **Concept drift**: The decision boundary change, i.e., $P(y/X)$ changes but $P(X)$ remains the same.\n",
    "$$\n",
    "   P(X_{train}) = P(X_{test}) \\\\\n",
    "   P(y_{train}|X_{train}) \\not =  P(y_{test}|X_{test})\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Detecting Model Drift using TorchDrift**  \n",
    "**Types of Detectors available in TorchDrift**\n",
    "\n",
    "TorchDrift is a PyTorch-based library with several drift detection methods implemented. The 5 different techniques according to the official documentation are given below:\n",
    "\n",
    "1. **Drift detector based on (multiple) Kolmogorov-Smirnov tests.**\n",
    "2. **Drift detector based on the kernel Maximum Mean Discrepancy (MMD) test.**\n",
    "3. **Implements the kernel MMD two-sample test.**\n",
    "4. **Computes the p-value for the two-sided two-sample KS test from the D-statistic.**\n",
    "5. **Computes the two-sample two-sided Kolmorogov-Smirnov statistic.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> *It seems like this library was originally created to serve image based use-cases (for example: Detecting drifts in production plants or similar applications). Therefore the models are based on pytorch tensors, and the model training is done using Pytorch lightning. It is possible that new users might find it hard to adopt TorchDrift as a library in their use-cases, as they will have to learn both Pytorch and Pytorch lightning to get started. It is also possible that a user is only interested in using the different detectors implemented. Therefore, I intend to show you a tutorial where I will walk you through converting a numpy array into a torch tensor, and focus on making practical usage of the ‘Drift detectors’, without going into other details which are not needed for simple machine learning use-cases.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 different techniques for detecting model drift within TorchDrift. In this blog post, we will focus on one of the most important detector: The kernel maximum mean discrepancy (MMD) detector."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Maths behind Maximum Mean Discrepancy (MMD) drift**\n",
    "**_`Maximum Mean Discrepancy (MMD)`_** is a statistic that can be used to assert if two distributions P and Q are coming from the same distribution or not. The underlying algorithm in brief is as follows:\n",
    "\n",
    "1. **Embed P and Q, the two datasets, in a Hilbert space, where the computation is easier and data is separable.**\n",
    "2. **Compute the centroids of P and Q in the Hilbert space.**\n",
    "3. **Measure their distance using the norm.**\n",
    "\n",
    "$$\n",
    "    MMD(\\mathcal P, \\mathcal Q) = \n",
    "    {\\lVert \n",
    "        E_{X \\sim \\mathcal P} [\\varphi (X)] -\n",
    "        E_{Y \\sim \\mathcal Q} [\\varphi (Y)]        \n",
    "        \\rVert}_H\n",
    "$$\n",
    "\n",
    "> A **_Hilbert space_** helps to generalize from the finite dimensional space to a infinite dimensional space."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset used and the computation of statistical differences in the distribution**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TorchDrift on tabular data**  \n",
    "\n",
    "The ‘Penguins’ dataset from the seaborn library is selected for performing experiments so that we can focus more on the drift detection part and less on the data itself. The data contains 7 columns with 344 instances each.\n",
    "\n",
    "As a first step, let’s load the data from the seaborn library to examine the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing _imaging: 지정된 모듈을 찾을 수 없습니다.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\matplotlib\\__init__.py:131\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpackaging\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mversion\u001b[39;00m \u001b[39mimport\u001b[39;00m parse \u001b[39mas\u001b[39;00m parse_version\n\u001b[0;32m    129\u001b[0m \u001b[39m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[0;32m    132\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m sanitize_sequence\n\u001b[0;32m    133\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_api\u001b[39;00m \u001b[39mimport\u001b[39;00m MatplotlibDeprecationWarning\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\matplotlib\\rcsetup.py:27\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mimport\u001b[39;00m _api, cbook\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcbook\u001b[39;00m \u001b[39mimport\u001b[39;00m ls_mapper\n\u001b[1;32m---> 27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m Colormap, is_color_like\n\u001b[0;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_fontconfig_pattern\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[0;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_enums\u001b[39;00m \u001b[39mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\matplotlib\\colors.py:51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumbers\u001b[39;00m \u001b[39mimport\u001b[39;00m Number\n\u001b[0;32m     50\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m     52\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mPIL\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mPngImagePlugin\u001b[39;00m \u001b[39mimport\u001b[39;00m PngInfo\n\u001b[0;32m     54\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmpl\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\PIL\\Image.py:100\u001b[0m\n\u001b[0;32m     91\u001b[0m MAX_IMAGE_PIXELS \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(\u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m4\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m3\u001b[39m)\n\u001b[0;32m     94\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[39m# If the _imaging C module is not present, Pillow will not load.\u001b[39;00m\n\u001b[0;32m     96\u001b[0m     \u001b[39m# Note that other modules should not refer to _imaging directly;\u001b[39;00m\n\u001b[0;32m     97\u001b[0m     \u001b[39m# import Image and use the Image.core variable instead.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m     \u001b[39m# Also note that Image.core is not a publicly documented interface,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[39m# and should be considered private and subject to change.\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _imaging \u001b[39mas\u001b[39;00m core\n\u001b[0;32m    102\u001b[0m     \u001b[39mif\u001b[39;00m __version__ \u001b[39m!=\u001b[39m \u001b[39mgetattr\u001b[39m(core, \u001b[39m\"\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    103\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[0;32m    104\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mThe _imaging extension was built for another version of Pillow or PIL:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    105\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCore version: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mgetattr\u001b[39m(core,\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39mPILLOW_VERSION\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m \u001b[39m\u001b[39mNone\u001b[39;00m)\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPillow version: \u001b[39m\u001b[39m{\u001b[39;00m__version__\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         )\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _imaging: 지정된 모듈을 찾을 수 없습니다."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "penguins = sns.load_dataset(\"penguins\")\n",
    "penguins.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 numerical attributes, each describing the bill length, bill depth, flipper length, and body mass.\n",
    "\n",
    "Let’s now have a look at the pairwise correlation between each of these numerical attributes, and what is the difference in their distribution per different classes of penguin species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "sns.pairplot(penguins[[\n",
    "    \"species\",\n",
    "    \"bill_length_mm\",\n",
    "    \"bill_depth_mm\",\n",
    "    \"flipper_length_mm\",\n",
    "    'body_mass_g',\n",
    "]], hue = 'species', size = 2.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it is very clear that the statistical distribution of the attributes is different for some of the attributes, when subsetted by ‘species’. Let’s write a code to split the data into a train set and test set and look at this difference in distribution more closely for one of these attributes for example ‘flipper_length_mm’."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the data into train-test sets. Note here that, I am splitting the data into train-test tests of 50% each, considering the low number of data records we have. If you plan to build a machine learning model on this dataset, it might be better to split it by Train: 67% and Test: 33%, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test set by 50-50 ratio\n",
    "train_set = penguins[\"flipper_length_mm\"][0:172].values\n",
    "test_set = penguins[\"flipper_length_mm\"][172:].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s look at the difference in distribution specifically for this attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def density_plot(train_set, test_set):\n",
    "    '''\n",
    "        Generate a density plot for two 1-D numpy arrays\n",
    "    '''\n",
    "\n",
    "    dataset_tensor = pd.DataFrame({'train_set' : train_set,\n",
    "                                'test_set' : test_set,})\n",
    "    dataset_tensor.plot.kde()\n",
    "    plt.xlabel('Values', fontsize = 22)\n",
    "    plt.ylabel('Density', fontsize = 22)\n",
    "\n",
    "density_plot(train_set, test_set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above plot, it looks like the train and test sets are different in their statistical distribution. But, **_how do we know if it is statistically significant?_** Let’s use TorchDrift to estimate the difference."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have to implement 4 functions here :\n",
    "\n",
    "1. **numpy_to_tensor** — To convert numpy data into a tensor object\n",
    "2. **drift_detector** — To compute drift\n",
    "3. **calculate_drift** — To manipulate the data\n",
    "4. **plot_driftscore** — To plot the output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the drift detector, we will make use of **'torchdrift.detectors.KernelMMDDriftDetector'** with the default **Gaussian Kernal**. We will use a p-value threshold of 0.5 to estimate the significance of the difference in distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchdrift\n",
    "from torchdrift.detectors.mmd import GaussianKernel\n",
    "\n",
    "def numpy_to_tensor(trainset, testset):\n",
    "    ''' \n",
    "        Convert numpy array to torch tensor\n",
    "    '''\n",
    "    train_tensor = torch.from_numpy(trainset)\n",
    "    train_tensor = train_tensor.reshape(-1, 1)\n",
    "    test_tensor = torch.from_numpy(testset)\n",
    "    test_tensor = test_tensor.reshape(-1, 1)\n",
    "    return train_tensor, test_tensor\n",
    "\n",
    "def plot_driftscore(drift_score, train_set, test_set):\n",
    "    ''' \n",
    "        Convert numpy array to torch tensor\n",
    "    '''\n",
    "    fig = plt.figure()\n",
    "    gs = fig.add_gridspec(3, hspace = 0.5)\n",
    "    axs = gs.subplots(sharex = False, sharey = False)\n",
    "    axs[0].plot(train_set)\n",
    "    axs[0].set_title(\"Train data\")\n",
    "    axs[1].plot(test_set)\n",
    "    axs[1].set_title(\"Test data\")\n",
    "    axs[2].plot(drift_score, color = 'r',\n",
    "                marker = 'o', linestyle = ':',\n",
    "                linewidth = 2, markersize = 12)\n",
    "    axs[2].set_title('p-values')\n",
    "\n",
    "def drift_detector(traintensor, testtensor, kernel):\n",
    "    '''\n",
    "        Use torchdrift to calculate p-value for a given test set\n",
    "    '''\n",
    "    drift_detector = torchdrift.detectors.KernelMMDDriftDetector(kernel = kernel)\n",
    "    drift_detector.fit(x = traintensor)\n",
    "    p_val = drift_detector.compute_p_value(testtensor)\n",
    "    if p_val < 0.05:\n",
    "        print(f\"The test set p-value is: {p_val}. The distributions are different.\")\n",
    "    else:\n",
    "        print(f\"The test set p-value is: {p_val}. The distributions are not different.\")\n",
    "        \n",
    "    return p_val\n",
    "\n",
    "def calculate_drift(train_set, test_set, steps = 1e3, kernel = \"GaussianKernel\"):\n",
    "    ''' \n",
    "        Calculate drift given a train and test datasets\n",
    "    '''\n",
    "    train_set_tensor, test_set_tensor = numpy_to_tensor(train_set, test_set)\n",
    "    drift_score = []\n",
    "    i = 0\n",
    "    while i < len(test_set):\n",
    "        test_data = test_set_tensor[i:i + steps]\n",
    "        p_value = drift_detector(train_set_tensor, test_data, kernel)\n",
    "        drift_score.append(p_value)\n",
    "        i = i + steps\n",
    "\n",
    "    plot_driftscore(drift_score, train_set, test_set)\n",
    "\n",
    "    return drift_score\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will make use of the functions we wrote and pass the train set and test sets, in order to estimate the difference in our statistical distribution. The ‘calculate_drift’ function expects a train_set, test_set, steps, and kernel as the parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ‘steps’ parameter indicates if you want to test the ‘test_set’ data as a whole or in segments against the ‘train_set’. If you want to test the data as a whole then the ‘steps’ needed to be larger than the size of the ‘test_set’. Otherwise, you can choose a smaller size for example 50. This feature is quite useful in case you are working with time series data where you will use segments of the data for comparison rather than the data as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GaussianKernel()\n",
    "calculate_drift(train_set, \n",
    "            test_set, # test_set \n",
    "            steps=len(test_set) + 1, \n",
    "            kernel= kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the p-value threshold we set, the computed p-value ie. 0.0 is less than the threshold p-value ie. 0.05, meaning the two datasets are statistically different from each other. You can also observe this difference as a 1-D plot as shown below for both the train and test sets, to understand where the values are drastically changing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the ‘torchdrift.detectors.KernelMMDDriftDetector’ with any other detector in the ‘drift_detector’ function to test out other types of detectors.\n",
    "\n",
    "Let’s now use the above function to test the data in segments, with a steps parameter equal to 50. Setting this parameter allows us to test for every 50 points in the ‘test_set’, and to understand how different the considered segment of 50 data points from the ‘train_set’ as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GaussianKernel()\n",
    "calculate_drift(train_set, \n",
    "            test_set, \n",
    "            steps=50,\n",
    "            kernel= kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TorchDrift on timeseries data**  \n",
    "The NYC taxi passengers' time-series dataset is selected for performing experiments using the same functionalities written above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "taxi_df = pd.read_csv(\"https://zenodo.org/record/4276428/files/STUMPY_Basics_Taxi.csv?download=1\")\n",
    "taxi_df['value'] = taxi_df['value'].astype(np.float64)\n",
    "taxi_df.value.plot(figsize = (25, 5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3600 data points each describing the number of passengers in the taxi. I split the data into equal halves for the train and test set, each containing 1800 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = taxi_df.value[0:1800].values\n",
    "test_set = taxi_df.value[1800:].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A step size of 300 was used to segment the test data into 6 segments. This allows comparing each segment of 300 test data points to the whole 1800 data points in the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GaussianKernel()\n",
    "calculate_drift(train_set, \n",
    "            test_set, \n",
    "            steps=300,\n",
    "            kernel= kernel)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value plot shows that the 4th segment of the data from data points 900 to 1200 in the test set is different from the train set data. If you look at the test data (the 1-D plot in the middle), you will observe that there is a dip in the number of passengers during this period between data points 900 to 1200, validating our finding using TorchDrift. This is quite useful information in case you have an anomaly detection model where you wanted to observe the drift in the number of passengers.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Conclusion**\n",
    "In conclusion, we developed a set of functions to make use of the drift detection algorithm KernelMMDDriftDetector (MMD) implemented in the TorchDrift python library, to test the difference in distribution between two datasets. We tested the functions developed on two datasets namely the tabular dataset of Penguins, and the time series dataset on NYC taxi passengers.\n",
    "\n",
    "Using the “penguin” dataset, we estimated the statistical distribution between the flipper_length of the birds by splitting the data into equal train and test sets. We estimated the significance of the p-value and found it to be less than 0.05.\n",
    "\n",
    "Using the NYC taxi passenger dataset, we estimated the statistical distribution of passengers using the taxi for various time periods and identified a decrease in the number of passengers during a time period using a p-value threshold of less than 0.05.\n",
    "\n",
    "If you are working in a domain where either the data change continuously or you want to test the statistical difference between the train, validation, and test set, you can use the above functions without worrying about the underlying implementation within TorchDrift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robust_OD_port_infras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fc22f08d13bab0d316ed6f6a9e3bdfdce6296fd30c03dbcc19a1f7cbe247571"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
