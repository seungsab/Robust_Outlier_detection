{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from numpy import linalg\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import squareform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Define some hyperparameters***\n",
    "---\n",
    "$$\n",
    "    \\text{Hyperparamerters you should define}\n",
    "$$\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_hyperparameter():\n",
    "    # Define filename of dataset\n",
    "    fn_load = 'J_Dataset_1101_0630_outlier_3sig'\n",
    "\n",
    "    # # of skip in sample (for downsampling)\n",
    "    n_skip = 10\n",
    "\n",
    "    # Set initial Training samples\n",
    "    IDX_INIT_MODEL = 20000\n",
    "    # IDX_INIT_MODEL = 10000\n",
    "\n",
    "    # Set k-nearest neighbors in KNN\n",
    "    k = 10\n",
    "\n",
    "    # Set distance metric for kNN\n",
    "    dist_metric = 'euclidean'\n",
    "\n",
    "    # Significance level for comformal anomaly detection\n",
    "    # alpha = 0.95 # 95%\n",
    "    alpha = 0.997 # 99.7%\n",
    "\n",
    "    # Define type of anomaly detection\n",
    "    col_interest = ['CG_1', 'CG_2', 'TT_1','TT_2'] # Caisson #1\n",
    "    # col_interest = ['CG_3', 'CG_4', 'TT_3','TT_4'] # Caisson #2\n",
    "    return fn_load, n_skip, IDX_INIT_MODEL, k, dist_metric, alpha, col_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_dataset(fn_load = 'J_Dataset_1101_0630_outlier_3sig',\n",
    "    col_interest = ['Time', 'CG_1', 'CG_2', 'TT_1', 'TT_2']):\n",
    "    # 1. Import experimental dataset\n",
    "        with open(fn_load + '.pickle', 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        fn = data['fn']\n",
    "        damage_ind = data['damage_ind']\n",
    "        df1, df3 = data['data'][0], data['data'][1]\n",
    "\n",
    "        # 2. Set Dataset\n",
    "        if 0:\n",
    "            col_interest = ['Time', 'CG_1', 'CG_2', 'TT_1', 'TT_2']\n",
    "            df = df1[col_interest]\n",
    "            Label = df1.Label.values\n",
    "\n",
    "        else:\n",
    "            col_interest = ['Time', 'CG_3', 'CG_4', 'TT_3', 'TT_4']\n",
    "            df = df3[col_interest]\n",
    "            Label = df3.Label.values\n",
    "\n",
    "        X_all = df.values[:, 1:]\n",
    "        return X_all.astype(np.float64), Label, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Data Manager**\n",
    "A class to manage our experimental data set for recursive monitoring\n",
    "- Initial training set for initial baseline model\n",
    "- Update baseline model and manipulate training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager():\n",
    "\n",
    "    def __init__(self, X, Y, df, IDX_INIT_MODEL = 20000, n_skip = 1):\n",
    "        # 1. Save data into class\n",
    "        self.X, self.Y, self.df = X, Y, df\n",
    "        \n",
    "        # 2. Reduce the sequence of dataset (too many samples)\n",
    "        # (Optional, determined by n_skip)\n",
    "        self.reduce_sequence_by_skip(n_skip)\n",
    "        IDX_INIT_MODEL = int(IDX_INIT_MODEL/n_skip)\n",
    "        self.IDX_INIT_MODEL = IDX_INIT_MODEL\n",
    "\n",
    "        # 3. Find damage index as # index of sample\n",
    "        damage_ind = []\n",
    "        for ind_label in np.unique(self.Y_all):\n",
    "            if ind_label != 0:\n",
    "                ind_damage = np.where(self.Y_all == ind_label)[0][0]\n",
    "                damage_ind.append(ind_damage)\n",
    "        \n",
    "        self.damage_ind = damage_ind\n",
    "\n",
    "        # 4. For Allocation of memory\n",
    "        SIZE_ALL = self.X_all.shape[0]\n",
    "        \n",
    "        self.Is_anomaly = np.zeros((SIZE_ALL, 1))\n",
    "        self.Threshold = np.zeros((SIZE_ALL, 1))\n",
    "\n",
    "        # 5. Set Initial Traininigset and Testset\n",
    "        self.Xtrain = self.X_all[0:IDX_INIT_MODEL,:]\n",
    "        self.Xtest = self.X_all[IDX_INIT_MODEL:,:]\n",
    "    \"\"\"\n",
    "        Reduce massive sample due to computational issues. \n",
    "        In reality, it doesn't matter becauses of online implemtation.\n",
    "        @params\n",
    "            n_skip: int\n",
    "                A # of samples for the skip\n",
    "            dat: np.ndarray\n",
    "                A mxn array with m samples with n features\n",
    "        @return\n",
    "            dat: Reduced # of samples int(m/n_skip)\n",
    "    \"\"\"\n",
    "    def reduce_sequence_by_skip(self, n_skip):\n",
    "        if self.X.ndim == 1:\n",
    "            self.X_all = self.X[::n_skip,]\n",
    "        else:\n",
    "            self.X_all = self.X[::n_skip, :]\n",
    "\n",
    "        self.Y_all = self.Y[::n_skip,]\n",
    "        self.df = self.df.iloc[::n_skip, :]\n",
    "\n",
    "    \"\"\"\n",
    "        Create a line plot of mxn data with label in y for legend creation and title\n",
    "        @params\n",
    "            x: np.ndarray\n",
    "                A mxn array with m samples with n features\n",
    "            y: np.ndarray\n",
    "                A m array of labels\n",
    "            title: str\n",
    "                The title of the scatter plot\n",
    "        @return\n",
    "            The generated plot in case you want to plot over it\n",
    "    \"\"\"\n",
    "    def plot_line_raw_data(self):\n",
    "        # Plot scatter plot (Time index vs. Label)\n",
    "        color_type_str = ['blue', 'orange', 'red']\n",
    "\n",
    "        plt.figure(figsize = (10, 3), dpi = 200)\n",
    "        for label_ind in np.unique(self.Y_all):\n",
    "            indice_ = np.where(self.Y_all == label_ind)\n",
    "            plt.plot(self.df.Time.iloc[indice_], self.Y_all[indice_], marker = '.', color = color_type_str[label_ind])\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Label')\n",
    "        plt.gca().set_yticks([0, label_ind])\n",
    "        plt.grid(linestyle = ':')\n",
    "        if 'CG_1' in self.df.columns:\n",
    "            struct_type = 'Caisson #1'\n",
    "        else:\n",
    "            struct_type = 'Caisson #3'\n",
    "\n",
    "        plt.title(struct_type)\n",
    "        plt.show()\n",
    "\n",
    "        for col_ind in range(self.X_all.shape[1]):\n",
    "            plt.figure(figsize = (10, 3), dpi = 200)\n",
    "            for label_ind in np.unique(self.Y_all):\n",
    "                row_ind = np.where(self.Y_all == label_ind)\n",
    "                plt.plot(self.df.Time.iloc[row_ind], self.X_all[row_ind, col_ind].reshape(-1, 1),\n",
    "                        marker = '.', color = color_type_str[label_ind])\n",
    "            plt.xlabel('Time')\n",
    "            plt.ylabel(list(self.df.columns[1:])[col_ind])\n",
    "            plt.grid(linestyle = ':')\n",
    "            plt.title(struct_type)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **3. K-Nearest Neighbor**\n",
    "A simple way to calculate a conformal predictor score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighbors():\n",
    "    \"\"\"\n",
    "        A simple real-valued function to compute the conformal scores\n",
    "        Each conformal score is the average k-nearest neighbors according to a specified metric\n",
    "        @params\n",
    "            k: int\n",
    "                Determines k nearest neighbors\n",
    "            metric: str\n",
    "                distance metric (see scipy's pdist function for valid metrics)\n",
    "    \"\"\"\n",
    "    def __init__(self,k,metric='euclidean'):\n",
    "        self._k = k\n",
    "        self._metric = metric\n",
    "\n",
    "    \"\"\"\n",
    "        Returns a pairwise distance matrix\n",
    "        @params\n",
    "            x: np.ndarray\n",
    "                An m x n array with m samples and n dimensions\n",
    "    \"\"\"\n",
    "    def get_pairwise_distance_matrix(self,x):\n",
    "        distances = pdist(x,self._metric)\n",
    "        distance_matrix = squareform(distances)\n",
    "        return distance_matrix\n",
    "\n",
    "    \"\"\"\n",
    "        Returns the mean pairwise distance between the k'th nearest neighbors\n",
    "        @params\n",
    "            x: np.ndarray\n",
    "                An m x n array with m samples and n dimensions\n",
    "    \"\"\"\n",
    "    def __call__(self,x):\n",
    "        distance_matrix = self.get_pairwise_distance_matrix(x)\n",
    "        distance_matrix = np.sort(distance_matrix,axis=1)\n",
    "        assert self._k +1 < distance_matrix.shape[1],\\\n",
    "            print('K must be less than the number of data points (k={},num_samples={})'.format(self._k +1,distance_matrix.shape[1]))\n",
    "        return np.mean(distance_matrix[:,1:self._k+1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **4. Conformal Anomaly Detector (CAD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConformalAnomalyDetector():\n",
    "    \"\"\"\n",
    "    Conformal Anomaly Detector Class\n",
    "    @params\n",
    "        ICM: class\n",
    "            An object whose call operation should produce an array of conformal scores\n",
    "        z: tuple (len==2)\n",
    "            Each element is an (x,y) pair of the training set for CAD\n",
    "        significance: float\n",
    "            The significance level (must be between 0 and 1 exclusive)\n",
    "    \"\"\"\n",
    "    def __init__ (self, ICM, x, y = None, significance=0.05):\n",
    "        self._ICM = ICM\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        assert significance > 0 and significance < 1, \\\n",
    "            print('Significance must be in range (0,1).')\n",
    "        self._significance = significance\n",
    "        \n",
    "    \"\"\"\n",
    "    Return true or false if the test example are an anomaly\n",
    "    @params\n",
    "        test: np.ndarray\n",
    "            A 1xn test example where m is the number of test examples and n is the number of dimensions\n",
    "    @return: bool\n",
    "        True if test input is anomaly and false otherwise \n",
    "    \"\"\"\n",
    "    def testIfAnomaly(self,test):\n",
    "        conformal_set = np.concatenate((self.x,test))\n",
    "        conformal_scores = self._ICM(conformal_set)\n",
    "        p = np.sum(conformal_scores >= conformal_scores[-1]) / (self.x.shape[0]+1)\n",
    "        return p < self._significance\n",
    "\n",
    "    \"\"\"\n",
    "    Return array of true or false if the test examples are an anomaly\n",
    "    @params\n",
    "        test: np.ndarray\n",
    "            A mxn test example where m is the number of test examples and n is the number of dimensions\n",
    "    @return: np.ndarray\n",
    "        An mx1 array of true if test input is anomaly and false otherwise \n",
    "    \"\"\" \n",
    "    def __call__(self,anomalies):\n",
    "        isAnomaly = [self.testIfAnomaly(np.expand_dims(anomalies[i],axis=0)) for i in range(anomalies.shape[0])]\n",
    "        return isAnomaly\n",
    "\n",
    "    \"\"\"\n",
    "    Change significance level (hyper-parameter)\n",
    "    @params\n",
    "        significance: float\n",
    "            The significance level (must be between 0 and 1 exclusive)\n",
    "    \"\"\" \n",
    "    def set_significance(self,significance):\n",
    "        assert significance > 0 and significance < 1, \\\n",
    "            print('Significance must be in range (0,1).')\n",
    "        self._significance = significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Main**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 400 features, but MinMaxScaler is expecting 4 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\seung\\OneDrive\\바탕 화면\\Robust_Outlier_detection\\20230712_인천항_데이터_항만인프라\\Step2_Anomaly_detection_using_KNN.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seung/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Robust_Outlier_detection/20230712_%EC%9D%B8%EC%B2%9C%ED%95%AD_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%95%AD%EB%A7%8C%EC%9D%B8%ED%94%84%EB%9D%BC/Step2_Anomaly_detection_using_KNN.ipynb#X16sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(significances)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seung/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Robust_Outlier_detection/20230712_%EC%9D%B8%EC%B2%9C%ED%95%AD_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%95%AD%EB%A7%8C%EC%9D%B8%ED%94%84%EB%9D%BC/Step2_Anomaly_detection_using_KNN.ipynb#X16sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m    significance \u001b[39m=\u001b[39m significances[i]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/seung/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Robust_Outlier_detection/20230712_%EC%9D%B8%EC%B2%9C%ED%95%AD_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%95%AD%EB%A7%8C%EC%9D%B8%ED%94%84%EB%9D%BC/Step2_Anomaly_detection_using_KNN.ipynb#X16sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m    Xnew_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39;49mtransform(Xnew\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seung/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Robust_Outlier_detection/20230712_%EC%9D%B8%EC%B2%9C%ED%95%AD_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%95%AD%EB%A7%8C%EC%9D%B8%ED%94%84%EB%9D%BC/Step2_Anomaly_detection_using_KNN.ipynb#X16sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m    conformal_predictor\u001b[39m.\u001b[39mset_significance(significance) \u001b[39m# change significance\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/seung/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Robust_Outlier_detection/20230712_%EC%9D%B8%EC%B2%9C%ED%95%AD_%EB%8D%B0%EC%9D%B4%ED%84%B0_%ED%95%AD%EB%A7%8C%EC%9D%B8%ED%94%84%EB%9D%BC/Step2_Anomaly_detection_using_KNN.ipynb#X16sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m    isAnomaly \u001b[39m=\u001b[39m conformal_predictor(Xnew_scaled) \u001b[39m# test if anomamlies according to current CAD\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\seung\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\seung\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:508\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[39m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \n\u001b[0;32m    496\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[39m    Transformed data.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 508\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    509\u001b[0m     X,\n\u001b[0;32m    510\u001b[0m     copy\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcopy,\n\u001b[0;32m    511\u001b[0m     dtype\u001b[39m=\u001b[39;49mFLOAT_DTYPES,\n\u001b[0;32m    512\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mallow-nan\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    513\u001b[0m     reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    514\u001b[0m )\n\u001b[0;32m    516\u001b[0m X \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_\n\u001b[0;32m    517\u001b[0m X \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmin_\n",
      "File \u001b[1;32mc:\\Users\\seung\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\sklearn\\base.py:588\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    585\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 588\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    590\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\seung\\anaconda3\\envs\\Robust_OD_port_infras\\lib\\site-packages\\sklearn\\base.py:389\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    386\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 389\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    390\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    391\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    392\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 400 features, but MinMaxScaler is expecting 4 features as input."
     ]
    }
   ],
   "source": [
    "# Load experimental data\n",
    "fn_load, n_skip, IDX_INIT_MODEL, k, dist_metric, alpha, col_interest = set_hyperparameter()\n",
    "X, Y, df = Load_dataset(fn_load, col_interest)\n",
    "\n",
    "# Generate DataManager Class\n",
    "dat = DataManager(X, Y, df, IDX_INIT_MODEL, n_skip)\n",
    "# dat.plot_line_raw_data()\n",
    "\n",
    "# Define KNN model\n",
    "k_nearest_neighbor = KNearestNeighbors(k=10) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "\n",
    "# Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "Xtrain_scaled = scaler.fit_transform(dat.Xtrain)\n",
    "conformal_predictor = ConformalAnomalyDetector(ICM=k_nearest_neighbor,x = dat.Xtrain) # initialize CAD\n",
    "\n",
    "Xnew = dat.Xtest[:100]\n",
    "significances = [0.025,0.05]\n",
    "for i in range(len(significances)):\n",
    "   significance = significances[i]\n",
    "   Xnew_scaled = scaler.transform(Xnew.reshape(1, -1))\n",
    "   conformal_predictor.set_significance(significance) # change significance\n",
    "   isAnomaly = conformal_predictor(Xnew_scaled) # test if anomamlies according to current CAD\n",
    "   print(isAnomaly)\n",
    "   # title = 'CAD Visualization (significance level={})'.format(significance)\n",
    "   # data_generator.showAnomalies(anomalies,isAnomaly,block = i==len(significances)-1,title=title) # plot results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.Xtest[:100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     np.random.seed(123432) # set seed for reproducibility\n",
    "#     data_generator = DataGenerator(num_samples_per_class=25) # create 10 classes each with 25 samples\n",
    "#     k_nearest_neighbor = KNearestNeighbors(k=10) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "#     conformal_predictor = ConformalAnomalyDetector(ICM=k_nearest_neighbor,z=(data_generator.x,data_generator.y)) # initialize CAD\n",
    "#     anomalies = data_generator.create_anomaly(200) # Generate 200 anomalies\n",
    "\n",
    "#     significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "#     for i in range(len(significances)):\n",
    "#         significance = significances[i]\n",
    "#         conformal_predictor.set_significance(significance) # change significance\n",
    "#         isAnomaly = conformal_predictor(anomalies) # test if anomamlies according to current CAD\n",
    "#         title = 'CAD Visualization (significance level={})'.format(significance)\n",
    "#         data_generator.showAnomalies(anomalies,isAnomaly,block = i==len(significances)-1,title=title) # plot results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Construct KNN model with given data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#     scaler = MinMaxScaler()\n",
    "#     Xtrain_scaled = scaler.fit_transform(Xtrain)\n",
    "\n",
    "#     np.random.seed(123432) # set seed for reproducibility\n",
    "#     data_generator = DataGenerator(num_samples_per_class=25) # create 10 classes each with 25 samples\n",
    "#     k_nearest_neighbor = KNearestNeighbors(k=k, metric=dist_metric) # Initialize the ICM that uses k-nearest neighbors(k=10)\n",
    "    \n",
    "    \n",
    "#     conformal_predictor = ConformalAnomalyDetector(ICM=k_nearest_neighbor,z=(data_generator.x,data_generator.y)) # initialize CAD\n",
    "#     anomalies = data_generator.create_anomaly(200) # Generate 200 anomalies\n",
    "\n",
    "#     significances = [0.025,0.05,0.25,0.5] # see how different significance levels affect results\n",
    "#     for i in range(len(significances)):\n",
    "#         significance = significances[i]\n",
    "#         conformal_predictor.set_significance(significance) # change significance\n",
    "#         isAnomaly = conformal_predictor(anomalies) # test if anomamlies according to current CAD\n",
    "#         title = 'CAD Visualization (significance level={})'.format(significance)\n",
    "#         data_generator.showAnomalies(anomalies,isAnomaly,block = i==len(significances)-1,title=title) # plot results\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robust_OD_port_infras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
