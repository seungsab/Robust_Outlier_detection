{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **0. Import Libraries and define Options**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Import Libraries***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define some options***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set initial Training samples\n",
    "IDX_INIT_MODEL = 500\n",
    "\n",
    "# Optimal PC Selection Method\n",
    "Opt_PC_select_method = 'eigengap'\n",
    "\n",
    "# Significance level for Q & T2 score\n",
    "# alpha = 0.95\n",
    "alpha = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1. Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# load\n",
    "fn_load = 'J_Dataset_1101_0630_outlier_3sig'\n",
    "\n",
    "with open(fn_load + '.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "fn = data['fn']\n",
    "damage_ind = data['damage_ind']\n",
    "df1, df3 = data['data'][0], data['data'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2. Set Dataset (Train & Test)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    col_interest = ['Time', 'CG_1', 'CG_2', 'TT_1', 'TT_2']\n",
    "    df = df1[col_interest]\n",
    "    Label = df1.Label.values\n",
    "\n",
    "else:\n",
    "    col_interest = ['Time', 'CG_3', 'CG_4', 'TT_3', 'TT_4']\n",
    "    df = df3[col_interest]\n",
    "    Label = df3.Label.values\n",
    "\n",
    "X_all = df.values[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Find damage index as # index of sample***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_ind = []\n",
    "for ind_label in np.unique(Label):\n",
    "    if ind_label != 0:\n",
    "        ind_damage = np.where(Label == ind_label)[0][0]\n",
    "        damage_ind.append(ind_damage)\n",
    "damage_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot Ground Truth (label)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scatter plot (Time index vs. Label)\n",
    "color_type_str = ['blue', 'orange', 'red']\n",
    "\n",
    "plt.figure(figsize = (10, 3), dpi = 200)\n",
    "for label_ind in np.unique(Label):\n",
    "    indice_ = np.where(Label == label_ind)\n",
    "    plt.plot(df.Time.iloc[indice_], Label[indice_], marker = '.', color = color_type_str[label_ind])\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Label')\n",
    "plt.gca().set_yticks([0, label_ind])\n",
    "plt.grid(linestyle = ':')\n",
    "if 'CG_1' in df.columns:\n",
    "    struct_type = 'Caisson #1'\n",
    "else:\n",
    "    struct_type = 'Caisson #3'\n",
    "\n",
    "plt.title(struct_type)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define Training and Test Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set last index for normal state\n",
    "IDX_NORMAL = damage_ind[0] - 1\n",
    "\n",
    "if IDX_INIT_MODEL > IDX_NORMAL:\n",
    "    IDX_INIT_MODEL = IDX_NORMAL\n",
    "    print(f'Index for Normal state should be less than index of damage {damage_ind[0]}')\n",
    "\n",
    "# For Allocation of memory\n",
    "SIZE_ALL = X_all.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plot Rawdata in line plot***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_type_str = ['blue', 'orange', 'red']\n",
    "\n",
    "for col_ind in range(X_all.shape[1]):\n",
    "    plt.figure(figsize = (10, 3), dpi = 200)\n",
    "    for label_ind in np.unique(Label):\n",
    "        row_ind = np.where(Label == label_ind)\n",
    "        plt.plot(df.Time.iloc[row_ind], X_all[row_ind, col_ind].reshape(-1, 1),\n",
    "                marker = '.', color = color_type_str[label_ind])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(col_interest[col_ind + 1])\n",
    "    plt.grid(linestyle = ':')\n",
    "    plt.title(struct_type)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **3. Set Initial Dataset & Scaling (Standardization)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Initial Traininig Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define standardizer (scaling)\n",
    "scaler = StandardScaler()\n",
    "Xtrain = scaler.fit_transform(X_all[0:IDX_INIT_MODEL,:])\n",
    "\n",
    "plt.figure(figsize = (10, 5), dpi =150)\n",
    "plt.plot(Xtrain, marker = '.', label = col_interest[1:])\n",
    "plt.grid(linestyle = ':')\n",
    "plt.xlabel('# Index')\n",
    "plt.title(f'Stadardized Training data {struct_type}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Set Initial Test Dataset***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = X_all[IDX_INIT_MODEL:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4. Construct Baseline model using initial trainig dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Helper function for ftting PCA***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_PCA_given_data(X):\n",
    "    '''\n",
    "        - Input\n",
    "            X : Normalzied training samples [N(# samples)-by-f(# features)]\n",
    "\n",
    "        - Output\n",
    "            pca: fitted PCA model\n",
    "            explVar: exaplined variance for PCs\n",
    "            V: principal compoent vectors\n",
    "            n_comp: # retained PCs\n",
    "    '''\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA().fit(X)\n",
    "\n",
    "    # Explained variance of each features for PCA\n",
    "    expVars = pca.explained_variance_\n",
    "    \n",
    "    # Retained PCs\n",
    "    V = pca.components_\n",
    "    \n",
    "    # Singular values\n",
    "    S = pca.singular_values_\n",
    "\n",
    "    return pca, expVars, V, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Run PCA for initial training dataset***\n",
    "- pca: PCA class instance\n",
    "- expVARs: Explained variances ($=\\frac{S_i}{\\sum S_i}$)\n",
    "- V: Principal Component Vectors\n",
    "- S: Singular Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca, expVars, V, S = perform_PCA_given_data(Xtrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Select optimal # of PC: n_comp***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Opt_PC_select_method == 'eigengap':\n",
    "    # Select optimal # of PCs: eigengap technique\n",
    "    # % ref.1) The rotation of eigenvectors by a perturbation (1970)\n",
    "    # % ref.2) Adaptive data-derived anomaly detection in the activated... (2016)\n",
    "    n_comp = np.argmax(np.abs(np.diff(expVars)))\n",
    "\n",
    "print(f'# optimal PC: {n_comp + 1}')\n",
    "retaind_PCs = np.arange(n_comp + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plot exlained PCs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize = (10, 7))\n",
    "ax[0].bar(np.arange(0, S.shape[0]), S)\n",
    "ax[0].set_ylabel('Eigenvalue (SV)')\n",
    "# ax[0].bar(np.arange(0, expVars.shape[0]), expVars / sum(expVars))\n",
    "# ax[0].set_ylabel('Explained Variance')\n",
    "ax[0].set_xlabel('# PCs')\n",
    "ax[0].grid(linestyle = ':')\n",
    "\n",
    "\n",
    "ax[1].plot(np.arange(0, S.shape[0]-1), np.abs(np.diff(S)))\n",
    "ax[1].plot(n_comp, np.abs(np.diff(S))[n_comp], 'bo', label = 'Optimal # PC')\n",
    "ax[1].set_ylabel('Difference of Eigenvalue (Eigengap)')\n",
    "# ax[1].plot(np.arange(0, S.shape[0]-1), np.abs(np.diff(expVars)))\n",
    "# ax[1].plot(n_comp, np.abs(np.diff(expVars))[n_comp], 'bo', label = 'Optimal # PC')\n",
    "# ax[1].set_ylabel('Difference of Explained Variance')\n",
    "ax[1].set_xlabel('# PCs')\n",
    "ax[1].grid(linestyle = ':')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compute Monitroing metrics and their thresholds***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Project_X_into_PCs(X, V, retaind_PCs):\n",
    "    return np.matmul(X, V[retaind_PCs].T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Prject dataset into retained PCs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_proj = Project_X_into_PCs(Xtrain, V, retaind_PCs)\n",
    "X = Xtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    Q(X, P_{1:r}) = X(I - P_{1:r}P_{1:r}^T)X^T = \\| X - \\hat{X} \\|^2\n",
    "$$\n",
    "\n",
    "where $\\hat{X} = P_{1:r} X$\n",
    "- $X$: Standardized X\n",
    "- $P$: Principal component (PC) vectors\n",
    "- $r$: # of retained PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1.1) Compute Q-Statistics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q_statistics(X, X_proj, V, retaind_PCs):\n",
    "    return np.sqrt(np.sum(\n",
    "    (X - np.matmul(X_proj, V[retaind_PCs]))**2\n",
    "    , axis = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = compute_Q_statistics(X, X_proj, V, retaind_PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*1.2) Compute threshold of Q-Statistics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    Q_{alpha} = \\frac{\\theta_2}{2 \\theta_1}\\chi_{\\alpha}^2(h)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where,\n",
    "- $\\theta_1$: sample mean\n",
    "- $\\theta_2$: sample variance\n",
    "- $\\chi_{\\alpha}^2(h)$: chi-squared distribution with $h$ degree of freedom and the significance level $\\alpha$\n",
    "- $h = \\frac{2\\theta_1^2}{\\theta_2}$\n",
    "\n",
    "> Note it is based on normality assumption on Q-statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Threshold_Q(Q, alpha):\n",
    "    from scipy.stats.distributions import chi2\n",
    "    \n",
    "    theta1 = np.mean(Q)\n",
    "    theta2 = np.var(Q)\n",
    "    h = 2 * (theta1 ** 2) / theta2\n",
    "    chi_h = chi2.ppf(alpha, df=h)\n",
    "    Qlimit = theta2/(2*theta1) * chi_h\n",
    "    return Qlimit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Qlimit = compute_Threshold_Q(Q, alpha) # scalar\n",
    "Qlimit = np.ones_like(Q) * Qlimit # Make it `scalar of Qlimit` to `same vector size of training data (Q1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 3), dpi = 200)\n",
    "plt.plot(Q, '.')\n",
    "plt.plot(Qlimit, color = 'r', linestyle = '-')\n",
    "plt.grid(linestyle = ':')\n",
    "plt.xlabel('# Sample Index')\n",
    "plt.ylabel('Q-statistics')\n",
    "plt.title(f'Q-statistics $\\\\alpha = {alpha}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2) Hotellingâ€™s T2 Statistic**\n",
    "\n",
    "Now to calculate the T2 statistic, just transform each example.\n",
    "\n",
    "We calculate the SVD decomposition of the covariance matrix, and with that we can use the equation below to calculate the z_score to each example in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    z = \\Lambda_{1:r}^{-1/2}P_{1:r}^TX, \\\\\n",
    "    \\text{ } \\\\\n",
    "    T^2 =z^Tz\n",
    "$$\n",
    "\n",
    "where,\n",
    "- X: Standardized X\n",
    "- $\\Lambda$:\n",
    "- $P$: Principal component (PC) vectors\n",
    "- $r$: # of retained PCs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.1) Compute T2-Statistics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_T2_statistics(X_proj, S, retaind_PCs):\n",
    "    S_inverse = np.mat(np.diag(S[retaind_PCs])).I\n",
    "\n",
    "    T2 = np.empty((X_proj.shape[0], ), dtype=np.float64)\n",
    "    for i in range(X_proj.shape[0]):\n",
    "        value = np.matmul(np.matmul(X_proj[i], S_inverse), X_proj[i])\n",
    "        T2[i] = float(value)\n",
    "\n",
    "    return T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T2 = compute_T2_statistics(X_proj, S, retaind_PCs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*2.2) Compute threshold of T2-Statistics*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "    T_{\\alpha}^{2} = \\frac{m(n-1)(n+1)}{n(n-m)} F_{\\alpha}(m, n - m)\n",
    "$$\n",
    "\n",
    "where,\n",
    "- $n$: # of samples\n",
    "- $m$: # of retained PCs\n",
    "- $F_{\\alpha}(m, n-m)$: F-distribution with $r$ and $(n-m)$ degrees of freedom with significance level $\\alpha$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Threshold_T2(T2, retaind_PCs, alpha):\n",
    "    Ntrain, dim = T2.shape[0], retaind_PCs.shape[0]\n",
    "    t2limit = ((dim*(Ntrain-1))/ (Ntrain - dim)) * \\\n",
    "        scipy.stats.f.ppf(q=alpha, dfn=dim, dfd=Ntrain - dim)\n",
    "    \n",
    "    return t2limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2limit = compute_Threshold_T2(T2, retaind_PCs, alpha)\n",
    "t2limit = np.ones_like(T2) * t2limit # Make it `scalar of t2limit` to `same vector size of training data (T2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 3), dpi = 200)\n",
    "plt.plot(T2, '.')\n",
    "plt.plot(t2limit, color = 'r', linestyle = '-')\n",
    "plt.grid(linestyle = ':')\n",
    "plt.xlabel('# Sample Index')\n",
    "plt.ylabel('T2-statistics')\n",
    "plt.title(f'T2-statistics $\\\\alpha = {alpha}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluation Step (Monitoring Phase)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Plot result*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comformal prediction => stoppting criteria => based on significance level (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler\n",
    "# Xtrain\n",
    "# Xtest\n",
    "# X_scaled\n",
    "# pca, expVars, V, S\n",
    "\n",
    "# Define standardizer (scaling)\n",
    "# scaler = StandardScaler()\n",
    "# Xtrain = scaler.fit_transform(X_all[0:IDX_INIT_MODEL,:])\n",
    "damage_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q = compute_Q_statistics(X, X_proj, V, retaind_PCs) # output: Vector\n",
    "# Qlimit = compute_Threshold_Q(Q, alpha) # output: Scalar\n",
    "\n",
    "# T2 = compute_T2_statistics(X_proj, S, retaind_PCs) # output: Vector\n",
    "# t2limit = compute_Threshold_T2(T2, retaind_PCs, alpha) # output: Scalar"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Robust_OD_port_infras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
